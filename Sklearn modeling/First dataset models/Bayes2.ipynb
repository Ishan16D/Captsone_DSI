{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import stanfordnlp\n",
    "from textblob import TextBlob\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "\n",
    "\n",
    "# filepath = '../datasets/news_cleaned_2018_02_13.csv'\n",
    "# nlinesfile = 9408908\n",
    "\n",
    "# cols = ['type', 'title']\n",
    "\n",
    "# nlinesrandomsample = 5000000\n",
    "# lines2skip = np.random.choice(np.arange(1,nlinesfile+1), (nlinesfile-nlinesrandomsample), replace=False, )\n",
    "# df = pd.read_csv(filepath, skiprows=lines2skip, usecols= cols )\n",
    "\n",
    "\n",
    "# print(f\"Inital shape : {df.shape}\")\n",
    "\n",
    "# df_f = df[df['type'] == 'fake']\n",
    "# df_h = df[df['type'] == 'hate']\n",
    "# df_b = df[df['type'] == 'bias']\n",
    "# df_p = df[df['type'] == 'political']\n",
    "# df_r = df[df['type'] == 'reliable']\n",
    "\n",
    "# df = pd.concat([df_f, df_r, df_h, df_b, df_p], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = range(0, len(df.index))\n",
    "\n",
    "df.sample(10)\n",
    "\n",
    "print(f\"Subsection df shape : {df.shape}\")\n",
    "\n",
    "# df['type'].value_counts(normalize = True)\n",
    "\n",
    "df['label'] = df['type'].map({'reliable' : 0, 'political' : 0, 'bias' : 1, 'fake' : 1, 'hate' : 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.628708\n",
       "1    0.371292\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type     0.000000\n",
       "title    0.000085\n",
       "label    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return tokenizer.tokenize(x)\n",
    "\n",
    "df['tokens'] = df['title'].map(tokenize)\n",
    "\n",
    "def lemmatize(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "df['lemma'] = df['tokens'].map(lemmatize)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(max_df=0.8, min_df = 3, stop_words = 'english', ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.fit_transform(df['lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3018995, 855274)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8552462939097607\n",
      "0.8223899601059425\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "print(nb.score(X_train, y_train))\n",
    "print(nb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_grid = {\n",
    "    'n_estimators' : [5, 25, 50, 100],\n",
    "    'max_depth' : [2, 4, 6, 8],\n",
    "    'max_features' : [None, 3, 6]\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(rf, rf_grid, n_iter=25, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ishan\\Anaconda3\\envs\\nltktb\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "rs.fit(X_train, y_train)\n",
    "print(f\"The training score is {rs.score(X_train, y_train)}\")\n",
    "print(f\"The testing score is {rs.score(X_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
