{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposal: FakeNews Detection Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Can news articles be grouped into different levels of credibility using the language and text feature frequencies. This project will seek to build a classification model that can determine credibility as well as identify specific text features that are important in the determination. The model will be trained on vectorized word counts of news article titles, as well as body text if available. Success will be guided by the harmonic mean (f1 score) though recall and precision will also be taken into account. Baseline accuracy will be the relative proportions of the different groups of labels.\n",
    "\n",
    "##### Grouping criteria has yet to be determined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods and models\n",
    "\n",
    "- The raw text based data will be processed using a TFIDF Vectorizer and Count Vectorizer to extract text features\n",
    "- Multiple classification models will be trained to find which model is able to predict credibility with the greatest success (harmonic mean, recall, precision)\n",
    "    - Preliminary list of models\n",
    "        - Support Vector Classifier\n",
    "        - Logistic Regression\n",
    "        - Random Forest Classifier\n",
    "        - K Nearest Neighbors Classifier\n",
    "        - Multinomial and Gaussian Naive Bayes Classifiers\n",
    "- A randominzed search will be performed over each model to determine the best hyper parameters for classification\n",
    "- The logistic regression will be used specifically in order to see if certain words are important in determining the credibility of an article\n",
    "- A convolutional neural network will also be trained to classify articles and provide a likelyhood of being in each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risks and Assumptions\n",
    "\n",
    "- A key assumption that this analysis is making is that the labels in the training data correctly assigned\n",
    "    - This project will be using a dataset that was precollected thus I do not have direct control of the labelling process\n",
    "    - A way around this might be to do a separate unsupervized clustering process on the data to see how it matches the labels that I am provided with\n",
    "- A risk that follows from that assumption is that the model may be too reliant on domain\n",
    "    - Labels were assigned based on domain credibility rather than based on the content itself so the model will actually be looking for linguistic patterns in the way a domain creates content and is relying on those labels being accurate and fairly assigned\n",
    "    - The model might be biased towards and against certain domains\n",
    "- A risk is determining how this model will handle satire\n",
    "    - Removing satirical articles from the dataset is risky though including them may make it difficult for the model to determine between what is intentionally false for the purpose of satire and what is false for the purpose of misleading readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "\n",
    "### The FakeNewsCorpus provided by https://github.com/several27\n",
    "\n",
    "- Corpus link: https://github.com/several27/FakeNewsCorpus\n",
    "- Prelabeled csv file containing 9.5 million news articles\n",
    "- Fields include title text, content, author, url, domain, keywords/meta keywords, and summary\n",
    "- Labels (type)\n",
    "    - Fake\n",
    "    - Satire\n",
    "    - Bias\n",
    "    - Conspiracy\n",
    "    - State\n",
    "    - Junksci\n",
    "    - Hate\n",
    "    - Clickbait\n",
    "    - Unreliable\n",
    "    - Political\n",
    "    - Reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary EDA\n",
    "\n",
    "##### Data was too large to load into memory so a sampling method was implemented\n",
    "\n",
    "#### Sampling method skiprows\n",
    "\n",
    "```filename = 'news_cleaned_2018_02_13.csv'\n",
    "nlinesfile = 9408908\n",
    "nlinesrandomsample = 100000\n",
    "lines2skip = np.random.choice(np.arange(1,nlinesfile+1), (nlinesfile-nlinesrandomsample), replace=False)\n",
    "df = pd.read_csv(filename, skiprows=lines2skip)```\n",
    "\n",
    "Reads in a sample of size 100,000 from the csv skipping a randomized list of row indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in sample\n",
    "\n",
    "**Warning**\n",
    "csv file moved to outside of the repository due to size issues- reading the file using this exact code will not be replicable.\n",
    "Edit filename in the read function to the desired filepath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "pd.options.display.max_rows = 999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9097, 17)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "filepath = '../datasets/news_cleaned_2018_02_13.csv'\n",
    "nlinesfile = 9408908\n",
    "nlinesrandomsample = 10000\n",
    "lines2skip = np.random.choice(np.arange(1,nlinesfile+1), (nlinesfile-nlinesrandomsample), replace=False)\n",
    "df = pd.read_csv(filepath, skiprows=lines2skip)\n",
    "\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Unnamed: 0'], inplace= True)\n",
    "df = df.reindex(index = range(0, df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2210</td>\n",
       "      <td>beehivebugle.com</td>\n",
       "      <td>satire</td>\n",
       "      <td>http://beehivebugle.com/2014/04/07/a-former-mo...</td>\n",
       "      <td>Hi there, I’ve got a wonderful message that I ...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>A Former Mormon’s 1st Annual Objective Respons...</td>\n",
       "      <td>View All Posts Lawrence B. Riff, Lawrence B. Riff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>With flagrant zeal you enter the Conference Ce...</td>\n",
       "      <td>Jesus, heavenly father, Musician, Food, LDS Ch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2388</td>\n",
       "      <td>christianpost.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>https://www.christianpost.com/news/changing-cu...</td>\n",
       "      <td>WASHINGTON — Building a culture that values li...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Changing a Culture of Self to a Culture of Life</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>Building a culture that values life will be a ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5133</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/gold-and-precious-met...</td>\n",
       "      <td>New Home Sales Explode Higher: Biggest Monthly...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>New Home Sales Explode Higher: Biggest Monthly...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6296</td>\n",
       "      <td>collectivelyconscious.net</td>\n",
       "      <td>junksci</td>\n",
       "      <td>http://collectivelyconscious.net/tag/america/</td>\n",
       "      <td>On the first part of the journey I was looking...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>Hive Mind for the Awakened</td>\n",
       "      <td>Joe Rogan, Revolution, Basic Income, Food &amp; Ag...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6963</td>\n",
       "      <td>naturalnews.com</td>\n",
       "      <td>junksci</td>\n",
       "      <td>https://www.naturalnews.com/048620_glyphosate_...</td>\n",
       "      <td>Impact on pineal gland, gut health explained\\n...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Glyphosate could combine with aluminum to incr...</td>\n",
       "      <td>Friday, February, Jennifer Lilley</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['glyphosate', 'aluminum', 'gut flora', 'alumi...</td>\n",
       "      <td>Glyphosate could combine with aluminum to incr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                     domain      type  \\\n",
       "0  2210           beehivebugle.com    satire   \n",
       "1  2388          christianpost.com  reliable   \n",
       "2  5133          beforeitsnews.com      fake   \n",
       "3  6296  collectivelyconscious.net   junksci   \n",
       "4  6963            naturalnews.com   junksci   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://beehivebugle.com/2014/04/07/a-former-mo...   \n",
       "1  https://www.christianpost.com/news/changing-cu...   \n",
       "2  http://beforeitsnews.com/gold-and-precious-met...   \n",
       "3      http://collectivelyconscious.net/tag/america/   \n",
       "4  https://www.naturalnews.com/048620_glyphosate_...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Hi there, I’ve got a wonderful message that I ...   \n",
       "1  WASHINGTON — Building a culture that values li...   \n",
       "2  New Home Sales Explode Higher: Biggest Monthly...   \n",
       "3  On the first part of the journey I was looking...   \n",
       "4  Impact on pineal gland, gut health explained\\n...   \n",
       "\n",
       "                   scraped_at                 inserted_at  \\\n",
       "0  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "1  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "2  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "3  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "4  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "\n",
       "                   updated_at  \\\n",
       "0  2018-02-02 01:19:41.756664   \n",
       "1  2018-02-02 01:19:41.756664   \n",
       "2  2018-02-02 01:19:41.756664   \n",
       "3  2018-02-02 01:19:41.756664   \n",
       "4  2018-02-02 01:19:41.756664   \n",
       "\n",
       "                                               title  \\\n",
       "0  A Former Mormon’s 1st Annual Objective Respons...   \n",
       "1    Changing a Culture of Self to a Culture of Life   \n",
       "2  New Home Sales Explode Higher: Biggest Monthly...   \n",
       "3                                            America   \n",
       "4  Glyphosate could combine with aluminum to incr...   \n",
       "\n",
       "                                             authors  keywords  \\\n",
       "0  View All Posts Lawrence B. Riff, Lawrence B. Riff       NaN   \n",
       "1                                                NaN       NaN   \n",
       "2                                                NaN       NaN   \n",
       "3                                                NaN       NaN   \n",
       "4                  Friday, February, Jennifer Lilley       NaN   \n",
       "\n",
       "                                       meta_keywords  \\\n",
       "0                                               ['']   \n",
       "1                                               ['']   \n",
       "2                                               ['']   \n",
       "3                                               ['']   \n",
       "4  ['glyphosate', 'aluminum', 'gut flora', 'alumi...   \n",
       "\n",
       "                                    meta_description  \\\n",
       "0  With flagrant zeal you enter the Conference Ce...   \n",
       "1  Building a culture that values life will be a ...   \n",
       "2                                                NaN   \n",
       "3                         Hive Mind for the Awakened   \n",
       "4  Glyphosate could combine with aluminum to incr...   \n",
       "\n",
       "                                                tags  summary source  \n",
       "0  Jesus, heavenly father, Musician, Food, LDS Ch...      NaN    NaN  \n",
       "1                                                NaN      NaN    NaN  \n",
       "2                                                NaN      NaN    NaN  \n",
       "3  Joe Rogan, Revolution, Basic Income, Food & Ag...      NaN    NaN  \n",
       "4                                                NaN      NaN    NaN  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain columns have a lot of null values.\n",
    "\n",
    "The important columns, type and title, are each less than 5% null so those rows can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                  0.000000\n",
       "domain              0.000000\n",
       "type                0.048587\n",
       "url                 0.000000\n",
       "content             0.000000\n",
       "scraped_at          0.000000\n",
       "inserted_at         0.000000\n",
       "updated_at          0.000000\n",
       "title               0.009124\n",
       "authors             0.450588\n",
       "keywords            1.000000\n",
       "meta_keywords       0.041772\n",
       "meta_description    0.532373\n",
       "tags                0.770254\n",
       "summary             1.000000\n",
       "source              0.784215\n",
       "dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Won't work if they are put in the same filter\n",
    "df = df[df['type'].isna() == False]\n",
    "df = df[df['title'].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                  0.000000\n",
       "domain              0.000000\n",
       "type                0.000000\n",
       "url                 0.000000\n",
       "content             0.000000\n",
       "scraped_at          0.000000\n",
       "inserted_at         0.000000\n",
       "updated_at          0.000000\n",
       "title               0.000000\n",
       "authors             0.446570\n",
       "keywords            1.000000\n",
       "meta_keywords       0.044330\n",
       "meta_description    0.543047\n",
       "tags                0.779865\n",
       "summary             1.000000\n",
       "source              0.770999\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the type (label) counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reliable      0.232851\n",
       "political     0.212902\n",
       "bias          0.142324\n",
       "fake          0.110826\n",
       "conspiracy    0.094610\n",
       "rumor         0.056346\n",
       "unknown       0.042231\n",
       "unreliable    0.037214\n",
       "clickbait     0.029748\n",
       "junksci       0.017032\n",
       "satire        0.014116\n",
       "hate          0.009799\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['type'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample percentages vs full dataset percentages (as provided by origin repository)\n",
    "\n",
    "| Label      | Sample | Full |\n",
    "|------------|--------|------|\n",
    "| reliable   | 23.3%    | 20%   |\n",
    "| political  | 21.3%   | 26%  |\n",
    "| bias       | 14.2%   | 14%  |\n",
    "| fake       | 11.1%   | 10%   |\n",
    "| conspiracy | 9.5%   | 9%  |\n",
    "| rumor      | 5.6%   | NA   |\n",
    "| unknown    | 4.2%   | NA   |\n",
    "| unreliable | 3.7%   | 3%  |\n",
    "| clickbait  | 2.9%   | 3%  |\n",
    "| junksci    | 1.7%   | 2%  |\n",
    "| satire     | 1.4%   | 1%  |\n",
    "| hate       | 0.9%   | 1%  |\n",
    "\n",
    "They are very close aside from small differences.\n",
    "It can be seen though that in the full dataset as well as the sample the 12 classes are unbalanced.\n",
    "Another concern is that the rumor and unknown labels are not listed in the github repository and data dictionary.\n",
    "\n",
    "Removing labels\n",
    "\n",
    "- Rumor and unknown\n",
    "    - Not listed in data dictionary\n",
    "- Satire\n",
    "    - May complicate initial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['type'] != 'rumor']\n",
    "df = df[df['type'] != 'unknown']\n",
    "df = df[df['type'] != 'satire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reliable      0.262424\n",
       "political     0.239942\n",
       "bias          0.160400\n",
       "fake          0.124901\n",
       "conspiracy    0.106626\n",
       "unreliable    0.041941\n",
       "clickbait     0.033526\n",
       "junksci       0.019195\n",
       "hate          0.011044\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['type'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining different labels into binary classification\n",
    "\n",
    "- 1 if reliable (~54%)\n",
    "    - reliable\n",
    "    - political\n",
    "    - clickbait\n",
    "- 0 if unreliable (~46%)\n",
    "    - unreliable\n",
    "    - conspiracy\n",
    "    - hate\n",
    "    - fake\n",
    "    - bias\n",
    "    - junksci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.535893\n",
       "0    0.464107\n",
       "Name: reliable, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reliable'] = df['type'].map({'reliable' : 1, 'political' : 1, 'clickbait' : 1, 'unreliable' : 0,\n",
    "                                 'conspiracy' : 0, 'hate' : 0, 'fake' : 0, 'bias' : 0, 'junksci' : 0})\n",
    "\n",
    "df['reliable'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering title data for basic model\n",
    "\n",
    "- Tokenizing and lemmatizing the article titles for use in a vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return tokenizer.tokenize(x)\n",
    "\n",
    "df['tokens'] = df['title'].map(tokenize)\n",
    "    \n",
    "def lemmatize(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "df['lemma'] = df['tokens'].map(lemmatize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8584    In California a Wary Eye on Hillsides a Rain F...\n",
       "2171    Jim DeMint leaving the Senate to head Heritage...\n",
       "2620             Hard reboot news article and information\n",
       "578                      Daily Kos Comments by cedar park\n",
       "7521                            Fleeting Glory Is No More\n",
       "7816                              Laurence Amir Jon Regen\n",
       "2655                             Earned Income Tax Credit\n",
       "6363                           Ancient Chinese secret huh\n",
       "5141          Pope writes to Muslims about mutual respect\n",
       "7596               Which Way Out of Our National Quagmire\n",
       "Name: lemma, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lemma'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words='english', ngram_range=(1, 2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7606x43818 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 77633 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec.fit_transform(df['title'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df = pd.DataFrame(data = cvec.vocabulary_.values(), index= cvec.vocabulary_.keys(), columns= ['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_df.sort_values(by = 'count', ascending=False).head(300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weirdly enough the top 300+ most common words and 2 word phrases in the corpus are in foreign languages.\n",
    "\n",
    "Trying with a higher minimum df threshhold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_2 = CountVectorizer(stop_words='english', ngram_range=(1, 2), min_df=.01 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_2.fit_transform(df['title'])\n",
    "\n",
    "vocab_df_2 = pd.DataFrame(data = cvec_2.vocabulary_.values(), index= cvec_2.vocabulary_.keys(), columns= ['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df_2.sort_values(by = 'count', ascending=False).head(100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a min_df of .01, meaning that a word must appear in at least 1% of the titles, removes that phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights from EDA\n",
    "\n",
    "- The sampling method is an effective way to get a balanced sample data frame that is possible to load into memory\n",
    "- Using a heavy handed approach to class combination, the binary reliable and unreliable classes are roughly equal\n",
    "    - Intend to spend more time developing more thought out class combinations\n",
    "    - Perhaps tiered multiclass classification\n",
    "- Min_df is an important hyper parameter that will be gridsearched over in a model\n",
    "    - Removing non english words could have negative effects on the model's ability to predict credibility\n",
    "- Next steps in EDA and feature engineering\n",
    "    - Begin to look into sentiment analysis\n",
    "    - Locate non english article titles\n",
    "    - Classify and handle satirical articles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
